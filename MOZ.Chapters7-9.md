# Chapter 7

For search engines that crawl the web, links are the streets between pages. Using sophisticated link analysis, the engines can discover how pages are related to each other.

Links aren’t everything in SEO but search professionals attribute a large portion of the engine’s algorithm to link-related factors. Through links, engines can analyze the popularity of websites based on the number and popularity of pages linking to them, and with metrics like trust, spam, and authority.

Trustworthy sites tend to link to other trusted sites, while spammy sites receive very few links from trusted sources. Authority models suggest that links are a very good way of identifying expert documents on a given subject.

Thanks to the focus on algorithmic use and analysis of links, growing the link profile of a website is critical to gaining traction, attention, and traffic from the engines. Link building is among the top tasks required for search ranking and traffic success.

## Link Signals

### Used by Search Engines

How do search engines assign values to links? 

1. Global popularity: the more popular and important a site is, the more links from that site matters. A site has thousands of diverse sites linking to it, which makes it a popular and important site. To earn trust and authority with the engines, you’ll need the help of other link partners. 

2. Local/Topic-specific popularity: links from sites within a topic-specific community matter more than links from general or off-topic sites. 

3. Anchor text: one of the strongest signals the engines use in rankings is anchor text. If dozens of links point to a page with the right keywords, that page has a very good probability of ranking well for the targeted phrase in that anchor text. 

4. Trust rank: some estimate that as much as 60% of the web’s pages are spam. In order to weed out irrelevant content, search engines use systems for measuring trust, many based on link graphs. Earning links from highly trusted domains can result in a significant boost in scoring metrics. Universities, government, and non-profit websites represents examples of high-trust domains.

5. Link neighborhood: a website that links to spam is likely spam itself, and in turn often has many spam sites linking back to it. By looking at these links, search engines can understand the “link neighborhood” in which your website exists. So be careful which sites you link to and which sites you attempt to earn links from.

6. Freshness: link signals tend to decay over time. Sites that were once popular often go stale, and eventually fail to earn new links. Thus, it’s important to continue earning additional links over time. 

7. Social sharing: although search engines treat socially shared links differently than other types of links, they notice them nonetheless. There is no denying the rising important of social channels.

## Link Building Basics

1. Natural Editorial Links: links that are given naturally by sites and pages that want to link to your content or company. These links require no action other than the creation of worthy material and the ability to create awareness about it.

2. Manual Outreach Link Building: SEO creates these links by emailing bloggers for links, submitting sites to directories, or paying for listings of any kind. The SEO often creates a value proposition by explaining to the link target why creating the link is in their best interest. 

3. Self-created, non-editorial: opportunities to create links through commits, profiles, and forum signatures. These links are considered spammy and should be pursued with caution.

It’s up to you to select which one of these will have the highest return on investment. As a general rule, it’s wise to build as vast and varied a link profile as possible, as this brings the best search engine results.

## Ad Campaigns

The value that link passes is diluted by the presence of other links on a page. Thus, being linked to a page with few links is better than being linked to a page with many links.

It takes time, practice, and experience to build comfort with these variables as they relate to search engine traffic. Use your web analytics to determine if your campaign was successful.

Success comes when you see increases in search traffic, higher rankings, more frequent search engines crawling and increasing referring link traffic. 

## Five Samples of Link Building Strategies

1. Get your customers to link to you: if you have loyal customers that love your brand, you can capitalize this by sending out partnership badges. 

2. Build a company blog; make it valuable, informative, and entertaining: this content and link strategy is so popular and valuable that it’s one of the few recommended personally by the engineers at Google. Blogs have the ability to contribute fresh material on a consistent basis, participate in conversations across the web, and earn listings and links from other blogs.

3. Create content that inspires viral sharing and natural thinking: users who see it want to share it with friends, and bloggers who see it will often share it through links. 

4. Be newsworthy: earn the attention of the press, bloggers, and news media. Sometimes this is giving away something for free, releasing a great new product, or stating something controversial.

5. An aside on buying links: Google discounts the influence of paid links. Link buying sometimes works but Moz recommend spending your time on long term link building strategies that focus on building links naturally.

# Chapter 8

SEOs tend to use a lot of tools. Some of them are provided by search engines themselves. These free resources provide data points and unique opportunities for exchanging information with the engines.

## Common Search Engine Protocols

1. Sitemaps: list of files that give hints tot eh search engines on who they can crawl your website. 

2. XML: this is the most widely accepted format for sitemaps. It is extremely easy for search engines to parse and can be produced by a plethora of sitemap generators.

3. RSS, easy to maintain, RSS is a dialect of XML, harder to manage due to updating properties.

The robots.txt file, a product of the Robots Exclusion Protocol is a file stored on a website’s root directory. It gives instructions to automated web crawlers visiting your site, including search crawlers.

Some commands available are:

1. Disallow: prevents robots from accessing specific pages or folders.
2. Sitemap: indicates the location of a website’s sitemap.
3. Crawl Delay: indicates the speed in which a robot can crawl a server.

Not all robots follow robot.txt. People can build bots that don’t follow this protocol and in some cases, can use it to identify the location of private information. 

## Meta Robots

The meta robots tag creates page-level instructions for search engine bots. This meta robots tag should be included in the head section of the HTML document.

<meta name=“ROBOTS” content=“NOINDEX, NOFOLLOW”>

Links act as votes. The rel=“nofollow” on a link will remove the vote for search engine purposes. rel=“canonical” will make the page refer to a href as the URL for search engine crawlers.

## Search Engine Tools

### Google Webmaster Tools

1. Geographic target: information that will help determine how that site appears in country specific search results.
2. Preferred domain: one that the webmaster specifies to be used to index their site’s page. 
3. URL parameter: indicated to Google about each parameter on your site such as “sort=price” to help Google crawl your site more efficiently.
4. Crawl rate: speed of crawl process.
5. Malware: Google will notify you if it found any malware.
6. Crawl errors: if Google encounters errors, it will report them.
7. HTML suggestions: Google looks for search engine-unfriendly HTML elements.

Search engines have only recently started providing better tools to help improve the search results. 

# Chapter 9

The best practice is to earn links from other sites. This will expose your content to the engines naturally.

### Meta Tags

Meta tags are no longer the central focus of SEO. Title tags, meta description tag, meta robot tags are crucial for quality SEO. Metas are not.

### Don’t keyword stuff(ing)

### Paid Search does not necessarily help bolster organic results

Spending on search engine advertising, PPC, does not necessarily improve organic SEO rankings. Google have erected walls in their organizations specifically to prevent this type of behavior.

### It is difficult to manipulate engines to artifically inflate rankings

Reasons:

1. It is not worth the effort: Google’s greatest product advantage is the ability to control and remove spam better than its competitors. It takes more effort to succeed with spam than producing good content, and the long term payoff with spam is nonexistent.

2. Smarter Engines: search engines are incredibly robust and can identify spam. Spam tactics usually results in search engines imposing penalties on your site.

### Page-level spam analysis

Keyword stuffing is littering keywords or phrases repetitively on a page in order to make it seem more relevant. It’s not challenging for a search engine to mark it as spam.

### Manipulative Linking

1. Reciprocal link exchange programs: links that point back and forth to one another. Engines are good at spotting these.

2. Link schemes: include link farms or link networks where websites are built purely as link sources to artificially inflate popularity. Easy to combat.

3. Paid links.

4. Low quality directory links: frequent source of manipulation. A large number of pay-for-placement web directories exist to serve this market and pass themselves off as legitimate. Google takes action against these sites by removing PageRank score from the toolbar.

### Cloaking

A basic tenet of search engine guidelines is to show the same content to the engine’s crawlers that you’d show to a human visitor. So, don’t hide text in  the HTML code of your website that a normal visitor can’t see.

When this guideline is broken, it is called “cloaking”. 

### Low Value Pages

Search Engines have methods to determine if a page provides unique content and value to its searchers. The most commonly filtered content are thin affiliate content, duplicate content, and dynamically generated content pages that provide very little unique text or value. 

### Content Value

An individual’s page is computed in part based on its uniqueness and the visitor experience. Likewise, the entire domain’s value is assessed. It’s not enough just to rank for a query. Once you’ve earned your ranking, you have to prove it over and over again.


